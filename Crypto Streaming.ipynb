{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crypto Streaming\n",
    "\n",
    "If you set up the upstream part of our streaming pipeline, you should have near real-time trading data of different cryptocurrencies being sent to different Kafka topics. In this notebook, we will read the trading value of our cryptocurrencies (in USD) and do some fun stuff with them!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Started (Imports & Setting Variables)\n",
    "\n",
    "First of all, to connect to Kafka from Pyspark, we need the right kind of extensions. These extensions are not built in, but luckily, using a neat trick we can define it within our notebook. More details: https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark and Structured Streaming related imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import TimestampType, StringType, IntegerType, StructType, StructField\n",
    "from pyspark.sql.functions import from_json, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a spark session\n",
    "spark = SparkSession.builder.appName(\"CryptoStreaming\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start reading a stream\n",
    "Spark's new structured streaming means we can stream the data straight into a dataframe! To do that, first we use the readStream to read a topic from Kafka like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read stream and subscribe to bitcoin topic\n",
    "df = spark.readStream \\\n",
    "          .format(\"kafka\") \\\n",
    "          .option(\"kafka.bootstrap.servers\", \"10.128.0.16:19092\") \\\n",
    "          .option(\"startingOffsets\", \"earliest\") \\\n",
    "          .option(\"subscribe\", \"BTC\") \\\n",
    "          .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind when we are reading the value from Kafka, we are also reading a lot of metadata that is internal to Kafka. You can take a look at these by using by using printSchema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also take a look at the raw content of the data received from Kafka. To do that, first we write a query to a new sql dataframe. This takes a snapshot of the stream, and it can be written to disk or save to memory for followup sql operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = df \\\n",
    "         .writeStream \\\n",
    "         .queryName(\"rawdata\")\\\n",
    "         .format(\"memory\")\\\n",
    "         .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+---------+------+--------------------+-------------+\n",
      "| key|               value|topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|     0|2019-10-17 19:24:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|     1|2019-10-17 19:24:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|     2|2019-10-17 19:25:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|     3|2019-10-17 19:26:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|     4|2019-10-17 19:26:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|     5|2019-10-17 19:27:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|     6|2019-10-17 19:28:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|     7|2019-10-17 19:28:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|     8|2019-10-17 19:29:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|     9|2019-10-17 19:30:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|    10|2019-10-17 19:30:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|    11|2019-10-17 19:31:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|    12|2019-10-17 19:32:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|    13|2019-10-17 19:32:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|    14|2019-10-17 19:33:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|    15|2019-10-17 19:34:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|    16|2019-10-17 19:35:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|    17|2019-10-17 19:35:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|    18|2019-10-17 19:36:...|            0|\n",
      "|null|[7B 22 74 69 6D 6...|  BTC|        0|    19|2019-10-17 19:37:...|            0|\n",
      "+----+--------------------+-----+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = spark.sql(\"select * from rawdata\")\n",
    "raw.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structuring The Value & Parsing JSON to Dataframe\n",
    "We can use the select expression to select the value column and also use the from_json function to parse the JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the value column\n",
    "raw_value_df = df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write stream to memory\n",
    "raw_value_query = raw_value_df.writeStream \\\n",
    "                              .queryName(\"raw_value\")\\\n",
    "                              .format(\"memory\")\\\n",
    "                              .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+\n",
      "|value                                                       |\n",
      "+------------------------------------------------------------+\n",
      "|{\"timestamp\": \"17-10-2019 19:24:08\", \"usd_value\": \"8014.91\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:24:44\", \"usd_value\": \"8014.91\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:25:21\", \"usd_value\": \"8014.91\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:26:04\", \"usd_value\": \"8018.71\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:26:45\", \"usd_value\": \"8018.71\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:27:26\", \"usd_value\": \"8018.71\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:28:08\", \"usd_value\": \"8017.44\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:28:50\", \"usd_value\": \"8018.16\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:29:31\", \"usd_value\": \"8018.16\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:30:12\", \"usd_value\": \"8018.07\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:30:54\", \"usd_value\": \"8016.51\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:31:35\", \"usd_value\": \"8016.03\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:32:16\", \"usd_value\": \"8017.77\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:32:58\", \"usd_value\": \"8015.45\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:33:39\", \"usd_value\": \"8015.53\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:34:20\", \"usd_value\": \"8016.71\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:35:02\", \"usd_value\": \"8016.55\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:35:43\", \"usd_value\": \"8016.09\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:36:24\", \"usd_value\": \"8014.42\"}|\n",
      "|{\"timestamp\": \"17-10-2019 19:37:06\", \"usd_value\": \"8015.75\"}|\n",
      "+------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the select statement to take snapshot of the query\n",
    "raw_value_query = spark.sql(\"select * from raw_value\")\n",
    "# print 20 values, False is so we can see the full value in the table\n",
    "raw_value_query.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to define the schema for parsing json value\n",
    "schema = StructType([StructField(\"timestamp\", StringType(), True),\n",
    "                     StructField(\"usd_value\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse json value and get bitcoin dataframe\n",
    "json_value_df = raw_value_df.selectExpr(\"cast (value as STRING) json_data\")\\\n",
    "                            .select(from_json(\"json_data\", schema).alias(\"bitcoin\"))\\\n",
    "                            .select(\"bitcoin.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to memory, take a snapshot, and show off our well-structured dataframe\n",
    "bitcoin_query = json_value_df.writeStream.format(\"memory\").queryName(\"bitcoin_value\").start()\n",
    "bitcoin_df = spark.sql(\"select * from bitcoin_value\")\n",
    "bitcoin_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
