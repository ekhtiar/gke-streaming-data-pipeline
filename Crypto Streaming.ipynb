{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crypto Streaming\n",
    "\n",
    "If you set up the upstream part of our streaming pipeline, you should have near real-time trading data of different cryptocurrencies being sent to different Kafka topics. In this notebook, we will read the trading value of our cryptocurrencies (in USD) and do some fun stuff with them!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Started (Imports & Setting Variables)\n",
    "\n",
    "First of all, to connect to Kafka from Pyspark, we need the right kind of extensions. These extensions are not built in, but luckily, using a neat trick we can define it within our notebook. More details: https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark and Structured Streaming related imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, window, max, min, avg\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import StringType, FloatType, StructType, StructField\n",
    "from pyspark.sql.functions import from_json, col, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a spark session\n",
    "spark = SparkSession.builder.appName(\"CryptoStreaming\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start reading a stream\n",
    "Spark's new structured streaming means we can stream the data straight into a dataframe! To do that, first we use the readStream to read a topic from Kafka like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read stream and subscribe to bitcoin topic\n",
    "df = spark.readStream \\\n",
    "          .format(\"kafka\") \\\n",
    "          .option(\"kafka.bootstrap.servers\", \"10.128.0.16:19092\") \\\n",
    "          .option(\"startingOffsets\", \"earliest\") \\\n",
    "          .option(\"subscribe\", \"BTC\") \\\n",
    "          .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind when we are reading the value from Kafka, we are also reading a lot of metadata that is internal to Kafka. You can take a look at these by using by using printSchema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also take a look at the raw content of the data received from Kafka. To do that, first we write a query to a new sql dataframe. This takes a snapshot of the stream, and it can be written to disk or save to memory for followup sql operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = df \\\n",
    "         .writeStream \\\n",
    "         .queryName(\"rawdata\")\\\n",
    "         .format(\"memory\")\\\n",
    "         .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "|key|value|topic|partition|offset|timestamp|timestampType|\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = spark.sql(\"select * from rawdata\")\n",
    "raw.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structuring The Value & Parsing JSON to Dataframe\n",
    "We can use the select expression to select the value column and also use the from_json function to parse the JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the value column\n",
    "raw_value_df = df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write stream to memory\n",
    "raw_value_query = raw_value_df.writeStream \\\n",
    "                              .queryName(\"raw_value\")\\\n",
    "                              .format(\"memory\")\\\n",
    "                              .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the select statement to take snapshot of the query\n",
    "raw_value_query = spark.sql(\"select * from raw_value\")\n",
    "# print 20 values, False is so we can see the full value in the table\n",
    "raw_value_query.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to define the schema for parsing json value\n",
    "schema = StructType([StructField(\"timestamp\", StringType(), True),\n",
    "                     StructField(\"usd_value\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse json value and get bitcoin dataframe\n",
    "json_value_df = raw_value_df.selectExpr(\"cast (value as STRING) json_data\")\\\n",
    "                            .select(from_json(\"json_data\", schema).alias(\"bitcoin\"))\\\n",
    "                            .select(\"bitcoin.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we could have parsed the data to the right format at the time we were writing the structure, it is often a good practice not to. By converting to string first and later converting to the right format here, we make our code a little bit more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to timestamp and integer\n",
    "json_value_df = json_value_df.withColumn('timestamp',to_timestamp(json_value_df.timestamp, 'dd-MM-yyyy HH:mm:ss'))\\\n",
    "                       .withColumn('usd_value', json_value_df.usd_value.cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- usd_value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out the schema\n",
    "json_value_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to memory, take a snapshot, and show off our well-structured dataframe\n",
    "bitcoin_query = json_value_df.writeStream.format(\"memory\").queryName(\"bitcoin_value\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|          timestamp|usd_value|\n",
      "+-------------------+---------+\n",
      "|2019-10-17 19:24:08|  8014.91|\n",
      "|2019-10-17 19:24:44|  8014.91|\n",
      "|2019-10-17 19:25:21|  8014.91|\n",
      "|2019-10-17 19:26:04|  8018.71|\n",
      "|2019-10-17 19:26:45|  8018.71|\n",
      "|2019-10-17 19:27:26|  8018.71|\n",
      "|2019-10-17 19:28:08|  8017.44|\n",
      "|2019-10-17 19:28:50|  8018.16|\n",
      "|2019-10-17 19:29:31|  8018.16|\n",
      "|2019-10-17 19:30:12|  8018.07|\n",
      "|2019-10-17 19:30:54|  8016.51|\n",
      "|2019-10-17 19:31:35|  8016.03|\n",
      "|2019-10-17 19:32:16|  8017.77|\n",
      "|2019-10-17 19:32:58|  8015.45|\n",
      "|2019-10-17 19:33:39|  8015.53|\n",
      "|2019-10-17 19:34:20|  8016.71|\n",
      "|2019-10-17 19:35:02|  8016.55|\n",
      "|2019-10-17 19:35:43|  8016.09|\n",
      "|2019-10-17 19:36:24|  8014.42|\n",
      "|2019-10-17 19:37:06|  8015.75|\n",
      "+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bitcoin_df = spark.sql(\"select * from bitcoin_value\")\n",
    "bitcoin_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How much did bitcoin price fluctuate in the last ten minutes? \n",
    "\n",
    "Now we have our dataframe in the right format, lets write some interesting queries. We will start of by answering the simple question, how much did the value of Bitcoin (in terms of USD) fluctuate in the last ten minutes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_minutes_ago_dt = (datetime.now() - timedelta(minutes=10))\n",
    "ten_mins_bitcoin_df = bitcoin_df.filter(bitcoin_df.timestamp > ten_minutes_ago_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_min_count = ten_mins_bitcoin_df.count()\n",
    "ten_min_max = ten_mins_bitcoin_df.agg({\"usd_value\": \"max\"}).collect()[0][0]\n",
    "ten_min_min = ten_mins_bitcoin_df.agg({\"usd_value\": \"min\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the last ten minutes, we received 14 updates and the price fluctuated 28.04 USD\n"
     ]
    }
   ],
   "source": [
    "print('In the last ten minutes, we received {0} updates and the price fluctuated {1:.2f} USD' .format(ten_min_count, ten_min_max - ten_min_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(usd_value)=7878.63)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ten_mins_bitcoin_df.agg({\"usd_value\": \"max\"}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window Functions\n",
    "Window functions are one of the most useful things in Spark's collection of arsenals. a window function computes a value for each and every row in a window. In our case, for example, if we want to know the max value or min value per 5 minutes interval, we can use a window function on our streaming data to get the answer.\n",
    "\n",
    "Some good further reading options:\n",
    "* https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-functions-windows.html\n",
    "* https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of data received per 5 minutes\n",
    "bitcoin_window_df = json_value_df.groupBy(window(json_value_df.timestamp, '5 minutes')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = bitcoin_window_df \\\n",
    "        .writeStream \\\n",
    "        .format(\"memory\") \\\n",
    "        .queryName(\"window_count\") \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-----+\n",
      "|window                                    |count|\n",
      "+------------------------------------------+-----+\n",
      "|[2019-10-17 19:45:00, 2019-10-17 19:50:00]|7    |\n",
      "|[2019-10-18 02:20:00, 2019-10-18 02:25:00]|7    |\n",
      "|[2019-10-18 04:05:00, 2019-10-18 04:10:00]|7    |\n",
      "|[2019-10-17 21:30:00, 2019-10-17 21:35:00]|7    |\n",
      "|[2019-10-18 03:20:00, 2019-10-18 03:25:00]|7    |\n",
      "|[2019-10-18 02:55:00, 2019-10-18 03:00:00]|8    |\n",
      "|[2019-10-18 00:20:00, 2019-10-18 00:25:00]|8    |\n",
      "|[2019-10-17 23:30:00, 2019-10-17 23:35:00]|7    |\n",
      "|[2019-10-18 00:35:00, 2019-10-18 00:40:00]|7    |\n",
      "|[2019-10-18 01:30:00, 2019-10-18 01:35:00]|7    |\n",
      "|[2019-10-18 04:00:00, 2019-10-18 04:05:00]|7    |\n",
      "|[2019-10-17 20:30:00, 2019-10-17 20:35:00]|7    |\n",
      "|[2019-10-18 03:00:00, 2019-10-18 03:05:00]|7    |\n",
      "|[2019-10-18 03:50:00, 2019-10-18 03:55:00]|8    |\n",
      "|[2019-10-17 23:55:00, 2019-10-18 00:00:00]|7    |\n",
      "|[2019-10-18 00:50:00, 2019-10-18 00:55:00]|7    |\n",
      "|[2019-10-18 05:00:00, 2019-10-18 05:05:00]|7    |\n",
      "|[2019-10-17 19:40:00, 2019-10-17 19:45:00]|7    |\n",
      "|[2019-10-18 03:05:00, 2019-10-18 03:10:00]|7    |\n",
      "|[2019-10-17 20:50:00, 2019-10-17 20:55:00]|7    |\n",
      "+------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bitcoin_window_df = spark.sql(\"select * from window_count\")\n",
    "bitcoin_window_df.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max per 5 minutes window\n",
    "bitcoin_window_df = json_value_df.groupBy(window(json_value_df.timestamp, '5 minutes')).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = bitcoin_window_df \\\n",
    "        .writeStream \\\n",
    "        .format(\"memory\") \\\n",
    "        .queryName(\"window_max\") \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+--------------+\n",
      "|window                                    |max(usd_value)|\n",
      "+------------------------------------------+--------------+\n",
      "|[2019-10-17 19:45:00, 2019-10-17 19:50:00]|8010.24       |\n",
      "|[2019-10-18 02:20:00, 2019-10-18 02:25:00]|8047.74       |\n",
      "|[2019-10-18 04:05:00, 2019-10-18 04:10:00]|8024.53       |\n",
      "|[2019-10-17 21:30:00, 2019-10-17 21:35:00]|8016.45       |\n",
      "|[2019-10-18 03:20:00, 2019-10-18 03:25:00]|8035.64       |\n",
      "|[2019-10-18 02:55:00, 2019-10-18 03:00:00]|8048.16       |\n",
      "|[2019-10-18 00:20:00, 2019-10-18 00:25:00]|8029.64       |\n",
      "|[2019-10-17 23:30:00, 2019-10-17 23:35:00]|8033.02       |\n",
      "|[2019-10-18 00:35:00, 2019-10-18 00:40:00]|8045.99       |\n",
      "|[2019-10-18 01:30:00, 2019-10-18 01:35:00]|8059.49       |\n",
      "|[2019-10-18 04:00:00, 2019-10-18 04:05:00]|8031.42       |\n",
      "|[2019-10-17 20:30:00, 2019-10-17 20:35:00]|8012.93       |\n",
      "|[2019-10-18 03:00:00, 2019-10-18 03:05:00]|8048.16       |\n",
      "|[2019-10-18 03:50:00, 2019-10-18 03:55:00]|8032.15       |\n",
      "|[2019-10-17 23:55:00, 2019-10-18 00:00:00]|8040.59       |\n",
      "|[2019-10-18 00:50:00, 2019-10-18 00:55:00]|8050.73       |\n",
      "|[2019-10-18 05:00:00, 2019-10-18 05:05:00]|8030.99       |\n",
      "|[2019-10-17 19:40:00, 2019-10-17 19:45:00]|8013.03       |\n",
      "|[2019-10-18 03:05:00, 2019-10-18 03:10:00]|8046.46       |\n",
      "|[2019-10-17 20:50:00, 2019-10-17 20:55:00]|8014.77       |\n",
      "+------------------------------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bitcoin_window_df = spark.sql(\"select * from window_max\")\n",
    "bitcoin_window_df.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max per 5 minutes window\n",
    "bitcoin_window_df = json_value_df.groupBy(window(json_value_df.timestamp, '5 minutes'))\\\n",
    "                        .agg(max('usd_value').alias('max'), min('usd_value').alias('min'), avg('usd_value').alias('avg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = bitcoin_window_df \\\n",
    "        .writeStream \\\n",
    "        .format(\"memory\") \\\n",
    "        .queryName(\"window_aggs\") \\\n",
    "        .outputMode(\"complete\") \\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-------+-------+------------------+\n",
      "|window                                    |max    |min    |avg               |\n",
      "+------------------------------------------+-------+-------+------------------+\n",
      "|[2019-10-17 19:45:00, 2019-10-17 19:50:00]|8010.24|8009.75|8010.0085714285715|\n",
      "|[2019-10-18 02:20:00, 2019-10-18 02:25:00]|8047.74|8042.5 |8046.20857142857  |\n",
      "|[2019-10-18 04:05:00, 2019-10-18 04:10:00]|8024.53|8017.61|8021.59142857143  |\n",
      "|[2019-10-17 21:30:00, 2019-10-17 21:35:00]|8016.45|8015.15|8015.694285714286 |\n",
      "|[2019-10-18 06:25:00, 2019-10-18 06:30:00]|7852.71|7842.62|7846.925          |\n",
      "|[2019-10-18 03:20:00, 2019-10-18 03:25:00]|8035.64|8035.25|8035.557142857143 |\n",
      "|[2019-10-18 02:55:00, 2019-10-18 03:00:00]|8048.16|8044.38|8046.2525000000005|\n",
      "|[2019-10-18 00:20:00, 2019-10-18 00:25:00]|8029.64|8018.77|8024.7325         |\n",
      "|[2019-10-17 23:30:00, 2019-10-17 23:35:00]|8033.02|8028.27|8030.597142857144 |\n",
      "|[2019-10-18 00:35:00, 2019-10-18 00:40:00]|8045.99|8036.41|8041.615714285714 |\n",
      "|[2019-10-18 01:30:00, 2019-10-18 01:35:00]|8059.49|8058.38|8059.260000000001 |\n",
      "|[2019-10-18 04:00:00, 2019-10-18 04:05:00]|8031.42|8024.73|8026.622857142857 |\n",
      "|[2019-10-17 20:30:00, 2019-10-17 20:35:00]|8012.93|8004.87|8008.855714285714 |\n",
      "|[2019-10-18 03:00:00, 2019-10-18 03:05:00]|8048.16|8045.12|8046.442857142857 |\n",
      "|[2019-10-18 03:50:00, 2019-10-18 03:55:00]|8032.15|8024.73|8026.078750000001 |\n",
      "|[2019-10-17 23:55:00, 2019-10-18 00:00:00]|8040.59|8028.03|8037.0142857142855|\n",
      "|[2019-10-18 00:50:00, 2019-10-18 00:55:00]|8050.73|8046.7 |8048.7814285714285|\n",
      "|[2019-10-18 05:00:00, 2019-10-18 05:05:00]|8030.99|8026.07|8029.695714285714 |\n",
      "|[2019-10-17 19:40:00, 2019-10-17 19:45:00]|8013.03|8009.75|8011.345714285714 |\n",
      "|[2019-10-18 03:05:00, 2019-10-18 03:10:00]|8046.46|8041.47|8044.541428571429 |\n",
      "+------------------------------------------+-------+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bitcoin_window_df = spark.sql(\"select * from window_aggs\")\n",
    "bitcoin_window_df.show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important concepts to read further on:\n",
    "* Checkpointing\n",
    "* Watermarking\n",
    "* Stream joins"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
