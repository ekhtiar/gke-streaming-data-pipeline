{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crypto Streaming\n",
    "\n",
    "If you set up the upstream part of our streaming pipeline, you should have near real-time trading data of different cryptocurrencies being sent to different Kafka topics. In this notebook, we will read the trading value of our cryptocurrencies (in USD) and do some fun stuff with them!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Started (Imports & Setting Variables)\n",
    "\n",
    "First of all, to connect to Kafka from Pyspark, we need the right kind of extensions. These extensions are not built in, but luckily, using a neat trick we can define it within our notebook. More details: https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.4 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark and Structured Streaming related imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.types import StringType, FloatType, StructType, StructField\n",
    "from pyspark.sql.functions import from_json, col, to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a spark session\n",
    "spark = SparkSession.builder.appName(\"CryptoStreaming\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start reading a stream\n",
    "Spark's new structured streaming means we can stream the data straight into a dataframe! To do that, first we use the readStream to read a topic from Kafka like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read stream and subscribe to bitcoin topic\n",
    "df = spark.readStream \\\n",
    "          .format(\"kafka\") \\\n",
    "          .option(\"kafka.bootstrap.servers\", \"10.128.0.16:19092\") \\\n",
    "          .option(\"startingOffsets\", \"earliest\") \\\n",
    "          .option(\"subscribe\", \"BTC\") \\\n",
    "          .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind when we are reading the value from Kafka, we are also reading a lot of metadata that is internal to Kafka. You can take a look at these by using by using printSchema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also take a look at the raw content of the data received from Kafka. To do that, first we write a query to a new sql dataframe. This takes a snapshot of the stream, and it can be written to disk or save to memory for followup sql operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = df \\\n",
    "         .writeStream \\\n",
    "         .queryName(\"rawdata\")\\\n",
    "         .format(\"memory\")\\\n",
    "         .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "|key|value|topic|partition|offset|timestamp|timestampType|\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "+---+-----+-----+---------+------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw = spark.sql(\"select * from rawdata\")\n",
    "raw.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structuring The Value & Parsing JSON to Dataframe\n",
    "We can use the select expression to select the value column and also use the from_json function to parse the JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the value column\n",
    "raw_value_df = df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write stream to memory\n",
    "raw_value_query = raw_value_df.writeStream \\\n",
    "                              .queryName(\"raw_value\")\\\n",
    "                              .format(\"memory\")\\\n",
    "                              .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the select statement to take snapshot of the query\n",
    "raw_value_query = spark.sql(\"select * from raw_value\")\n",
    "# print 20 values, False is so we can see the full value in the table\n",
    "raw_value_query.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to define the schema for parsing json value\n",
    "schema = StructType([StructField(\"timestamp\", StringType(), True),\n",
    "                     StructField(\"usd_value\", StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse json value and get bitcoin dataframe\n",
    "json_value_df = raw_value_df.selectExpr(\"cast (value as STRING) json_data\")\\\n",
    "                            .select(from_json(\"json_data\", schema).alias(\"bitcoin\"))\\\n",
    "                            .select(\"bitcoin.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to memory, take a snapshot, and show off our well-structured dataframe\n",
    "bitcoin_query = json_value_df.writeStream.format(\"memory\").queryName(\"bitcoin_value\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|          timestamp|usd_value|\n",
      "+-------------------+---------+\n",
      "|17-10-2019 19:24:08|  8014.91|\n",
      "|17-10-2019 19:24:44|  8014.91|\n",
      "|17-10-2019 19:25:21|  8014.91|\n",
      "|17-10-2019 19:26:04|  8018.71|\n",
      "|17-10-2019 19:26:45|  8018.71|\n",
      "|17-10-2019 19:27:26|  8018.71|\n",
      "|17-10-2019 19:28:08|  8017.44|\n",
      "|17-10-2019 19:28:50|  8018.16|\n",
      "|17-10-2019 19:29:31|  8018.16|\n",
      "|17-10-2019 19:30:12|  8018.07|\n",
      "|17-10-2019 19:30:54|  8016.51|\n",
      "|17-10-2019 19:31:35|  8016.03|\n",
      "|17-10-2019 19:32:16|  8017.77|\n",
      "|17-10-2019 19:32:58|  8015.45|\n",
      "|17-10-2019 19:33:39|  8015.53|\n",
      "|17-10-2019 19:34:20|  8016.71|\n",
      "|17-10-2019 19:35:02|  8016.55|\n",
      "|17-10-2019 19:35:43|  8016.09|\n",
      "|17-10-2019 19:36:24|  8014.42|\n",
      "|17-10-2019 19:37:06|  8015.75|\n",
      "+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bitcoin_df = spark.sql(\"select * from bitcoin_value\")\n",
    "bitcoin_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we could have parsed the data to the right format at the time we were writing the structure, it is often a good practice not to. By converting to string first and later converting to the right format here, we make our code a little bit more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to timestamp and integer\n",
    "bitcoin_df = bitcoin_df.withColumn('timestamp',to_timestamp(bitcoin_df.timestamp, 'dd-MM-yyyy HH:mm:ss'))\\\n",
    "                       .withColumn('usd_value', bitcoin_df.usd_value.cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- usd_value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out the schema\n",
    "bitcoin_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How much did bitcoin price fluctuate in the last ten minutes? \n",
    "\n",
    "Now we have our dataframe in the right format, lets write some interesting queries. We will start of by answering the simple question, how much did the value of Bitcoin (in terms of USD) fluctuate in the last ten minutes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_minutes_ago_dt = (datetime.now() - timedelta(minutes=10))\n",
    "ten_mins_bitcoin_df = bitcoin_df.filter(bitcoin_df.timestamp > ten_minutes_ago_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_min_count = ten_mins_bitcoin_df.count()\n",
    "ten_min_max = ten_mins_bitcoin_df.agg({\"usd_value\": \"max\"}).collect()[0][0]\n",
    "ten_min_min = ten_mins_bitcoin_df.agg({\"usd_value\": \"min\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the last ten minutes, we received 15 updates and the price fluctuated 5.32 USD\n"
     ]
    }
   ],
   "source": [
    "print('In the last ten minutes, we received {0} updates and the price fluctuated {1:.2f} USD' .format(ten_min_count, ten_min_max - ten_min_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
